# -*- coding: utf-8 -*-
"""DecisionTreeRegressor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iZwksl8utB-hQazyvPWRw0ph13L3TIbV
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv("student_data.csv")

df.head()

df.shape

df.info()

sns.histplot(df['G3'], kde=True)
plt.show()

sns.pairplot(df.select_dtypes(include="number"))
plt.show()

sns.scatterplot(x=df['school'], y=df['famsize'], hue=df['G3'])
plt.show()

df['famsize'].unique()

for col in df.columns:
    print(df[col].value_counts())

(df['romantic']).value_counts()

X = df.drop('G3', axis=1)
y = df['G3']

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=15)

from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

one_hot_encoder = OneHotEncoder(feature_name_combiner="concat")

scaler = StandardScaler()

one_hot_columns = ["school", "sex", "address", "famsize", "Pstatus", "Mjob", "Fjob", "reason", "guardian", "schoolsup", "famsup", "paid", "activities",
                   "nursery", "higher", "internet", "romantic"]

standard_scaler_columns = ["age", "Medu", "Fedu", "traveltime", "studytime", "failures", "famrel", "freetime", "goout",
                           "Dalc", "Walc", "health", "absences", "G1", "G2"]

preprocessor = ColumnTransformer(transformers= [
    ("ohe", one_hot_encoder, one_hot_columns), ("scaled", scaler, standard_scaler_columns)
], remainder = "drop")

X_train_transformed = preprocessor.fit_transform(X_train)
X_test_transformed = preprocessor.transform(X_test)

X_train_transformed_df = pd.DataFrame(X_train_transformed, columns=preprocessor.get_feature_names_out())

X_train_transformed_df.head()

"""## Decision Tree Regressor"""

from sklearn.tree import DecisionTreeRegressor

dt_regressor = DecisionTreeRegressor()

dt_regressor.fit(X_train_transformed, y_train)

y_pred = dt_regressor.predict(X_test_transformed)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

print("Mean Absolute Error :", mean_absolute_error(y_test, y_pred))
print("Mean Squared Error :", mean_squared_error(y_test, y_pred))
print("r2 Score :", r2_score(y_test, y_pred))

"""Hyperparameter Tuning"""

from sklearn.model_selection import GridSearchCV

params = {
    "criterion" : ["squared_error", "friedman_mse", "absolute_error", "poisson"],
    "splitter" : ["best", "random"],
    "max_depth" : [1, 2, 3, 4, 5, 10, 15, None],
    "max_features" : ["sqrt", "log2", None]
}

grid = GridSearchCV(estimator=DecisionTreeRegressor(), param_grid=params, scoring="r2", cv=5)

import warnings
warnings.filterwarnings("ignore")

grid.fit(X_train_transformed, y_train)

grid.best_score_

y_pred = grid.predict(X_test_transformed)

print("Mean Absolute Error :", mean_absolute_error(y_test, y_pred))
print("Mean Squared Error :", mean_squared_error(y_test, y_pred))
print("r2 Score :", r2_score(y_test, y_pred))

best_tree_model = DecisionTreeRegressor(criterion="friedman_mse", max_depth=4)
best_tree_model.fit(X_train_transformed, y_train)

y_pred = best_tree_model.predict(X_test_transformed)

print("Mean Absolute Error :", mean_absolute_error(y_test, y_pred))
print("Mean Squared Error :", mean_squared_error(y_test, y_pred))
print("r2 Score :", r2_score(y_test, y_pred))

plt.scatter(y_test, y_pred)
plt.show()

plt.figure(figsize=(12,8))

from sklearn import tree

df_columns = one_hot_columns + standard_scaler_columns

tree.plot_tree(best_tree_model.fit(X_train_transformed, y_train), feature_names = preprocessor.get_feature_names_out())

"""##Diğer Regresyon çeşitlerini deneyelim

Linear Regression
"""

from sklearn.linear_model import LinearRegression
linreg = LinearRegression()
linreg.fit(X_train_transformed, y_train)
y_pred = linreg.predict(X_test_transformed)
print("Mean Absolute Error :", mean_absolute_error(y_test, y_pred))
print("Mean Squared Error :", mean_squared_error(y_test, y_pred))
print("r2 Score :", r2_score(y_test, y_pred))

plt.scatter(y_test, y_pred)
plt.show()

"""Lasso"""

from sklearn.linear_model import Lasso
lasso = Lasso()
lasso.fit(X_train_transformed, y_train)
y_pred = lasso.predict(X_test_transformed)
print("Mean Absolute Error :", mean_absolute_error(y_test, y_pred))
print("Mean Squared Error :", mean_squared_error(y_test, y_pred))
print("r2 Score :", r2_score(y_test, y_pred))
plt.scatter(y_test, y_pred)
plt.show()

"""Ridge"""

from sklearn.linear_model import Ridge
ridge = Ridge()
ridge.fit(X_train_transformed, y_train)
y_pred = ridge.predict(X_test_transformed)
print("Mean Absolute Error :", mean_absolute_error(y_test, y_pred))
print("Mean Squared Error :", mean_squared_error(y_test, y_pred))
print("r2 Score :", r2_score(y_test, y_pred))
plt.scatter(y_test, y_pred)
plt.show()

"""Support Vector Regressor"""

from sklearn.svm import SVR
svr = SVR()
svr.fit(X_train_transformed, y_train)
y_pred = svr.predict(X_test_transformed)
print("Mean Absolute Error :", mean_absolute_error(y_test, y_pred))
print("Mean Squared Error :", mean_squared_error(y_test, y_pred))
print("r2 Score :", r2_score(y_test, y_pred))
plt.scatter(y_test, y_pred)
plt.show()

from sklearn.svm import SVR

params = {
    "kernel" : ["linear", "poly", "rbf", "sigmoid"],
    "gamma" : ["scale", "auto"],
    "C" : [0.01, 0.1, 1, 10, 100],
}

grid = GridSearchCV(estimator=SVR(), param_grid=params, scoring="r2", cv=5)

import warnings
warnings.filterwarnings("ignore")
grid.fit(X_train_transformed, y_train)

y_pred = grid.predict(X_test_transformed)
print("Mean Absolute Error :", mean_absolute_error(y_test, y_pred))
print("Mean Squared Error :", mean_squared_error(y_test, y_pred))
print("r2 Score :", r2_score(y_test, y_pred))
plt.scatter(y_test, y_pred)
plt.show()

"""KNN Regressor"""

from sklearn.neighbors import KNeighborsRegressor

params = {
    "n_neighbors" : [1, 2, 3, 4, 5, 15],
    "weights" : ["uniform", "distance", None],
    "algorithm" : ["auto", "ball_tree", "kd_tree"],
}

grid = GridSearchCV(estimator=KNeighborsRegressor(), param_grid=params, scoring="r2", cv=5)

import warnings
warnings.filterwarnings("ignore")
grid.fit(X_train_transformed, y_train)

y_pred = grid.predict(X_test_transformed)
print("Mean Absolute Error :", mean_absolute_error(y_test, y_pred))
print("Mean Squared Error :", mean_squared_error(y_test, y_pred))
print("r2 Score :", r2_score(y_test, y_pred))
plt.scatter(y_test, y_pred)
plt.show()